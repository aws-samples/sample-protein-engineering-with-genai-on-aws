{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 (inference): Generate protein sequences at scale with Progen2 on AWS Batch\n",
    "\n",
    "This notebook will guide you through defining Progen2 prompts with parameters, submitting Batch jobs, monitoring job status, and finally \n",
    "viewing generated sequences.\n",
    "\n",
    "#### Prerequisites\n",
    "- Progen2 docker image pushed to ECR\n",
    "- Batch resources provisioned (Compute Environment, Job Queue, Job Definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "First, let's get our AWS account information and set up variables we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Get AWS account information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Define S3 bucket and folder names\n",
    "S3_BUCKET = f'workshop-data-{account_id}'\n",
    "LAB1_FOLDER = 'lab1-progen'\n",
    "LAB2_FOLDER = 'lab2-amplify'\n",
    "LAB3_FOLDER = 'lab3-esmfold'\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Define model \n",
    "model_version = 'progen2-small'\n",
    "model_id = f'hugohrban/{model_version}' \n",
    "\n",
    "# Create batch client\n",
    "batch_client = boto3.client('batch')\n",
    "\n",
    "# Define Batch job queue and job definition \n",
    "job_queue_name = 'progen2-batch-job-queue'\n",
    "job_definition_name = 'progen2-job-definition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define prompts with parameters to generate protein sequences\n",
    "\n",
    "\n",
    "The inference parameters control how Progen2 generates protein sequences. Each configuration includes:\n",
    "\n",
    "**Key Parameters:**\n",
    "- *`prompt`*: Starting token sequence that serves as the seed for generation - the model continues from this initial sequence to generate longer protein sequences\n",
    "- *`max_length`*: Target sequence length\n",
    "- *`temperature`*: Controls randomness in token selection, where low values produce conservative sequences and high values generate more creative, diverse sequences\n",
    "- *`top_p`*: Considers only tokens whose cumulative probability reaches the specified threshold, adapting to the model's confidence level\n",
    "- *`top_k`*: Limits selection to a specified number of the most probable tokens at each position\n",
    "\n",
    "**Strategy:** 10 configurations with different parameters to generate sequences ranging from conservative (similar to training data) to creative (novel sequences)\n",
    "\n",
    "The parameters file will be stored on S3 for distributed batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/$LAB1_FOLDER/inference-params.json\n",
    "\n",
    "{\n",
    "    \"inference-params\": [\n",
    "        {\"prompt_id\": \"prompt-001\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.001, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-002\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.001, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-003\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.2, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-004\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.2, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-005\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.4, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-006\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.4, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-007\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.7, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-008\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.7, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-009\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.9, \"top_p\":0.9, \"top_k\":50},\n",
    "\n",
    "        {\"prompt_id\": \"prompt-010\", \"prompt\": \"MEVVIVTGMSGAGK\", \"max_length\":100, \n",
    "        \"temperature\": 0.9, \"top_p\":0.9, \"top_k\":50}\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/$LAB1_FOLDER/inference-params.json s3://$S3_BUCKET/$LAB1_FOLDER/inference-params.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate protein sequences with inference parameters\n",
    "\n",
    "AWS Batch excels at parallel processing, allowing you to scale protein sequence generation by distributing the workload across multiple concurrent jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Submit Batch jobs\n",
    "\n",
    "<pre>\n",
    "batch_count = 5  # defines how many jobs will be created\n",
    "batch_size  = 2  # defines how many sequences will be generated by each job\n",
    "</pre>\n",
    "\n",
    "\n",
    "This configuration will:\n",
    "- Create 5 concurrent Batch jobs \n",
    "- Generate 2 sequences per job (total of 10 sequences)\n",
    "- Complete faster than a single job processing all 10 sequences\n",
    "- Provide better fault tolerance and resource distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 5\n",
    "batch_size = 2\n",
    "\n",
    "s3_input_params_path = f's3://{S3_BUCKET}/{LAB1_FOLDER}/inference-params.json'\n",
    "s3_output_path = f's3://{S3_BUCKET}/{LAB1_FOLDER}/run-{batch_count}-{batch_size}'\n",
    "\n",
    "\n",
    "jobs = []\n",
    "for batchNumber in range(batch_count):\n",
    "\n",
    "    # Generate unique job name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    job_name = f'progen2-batch-job-{batchNumber}-{timestamp}'\n",
    "\n",
    "    # Submit the job\n",
    "    response = batch_client.submit_job(\n",
    "        jobName=job_name,\n",
    "        jobQueue=job_queue_name,\n",
    "        jobDefinition=job_definition_name,\n",
    "        parameters={\n",
    "            'hfModelId': model_id,\n",
    "            's3InputParamsPath': s3_input_params_path,\n",
    "            'batchId' : f'batch-10{batchNumber}',\n",
    "            'batchSize': f'{batch_size}',\n",
    "            'batchNumber': f'{batchNumber}',\n",
    "            's3OutputPath': s3_output_path\n",
    "        }\n",
    "    )\n",
    "    jobs.append(response)\n",
    "\n",
    "    job_id = response['jobId']\n",
    "    print(f\"Submitted job: {response['jobName']}\")\n",
    "    print(f\"   Job ID: {job_id}\")\n",
    "    print(f\"   Job ARN: {response['jobArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Wait for all jobs to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_jobs_completion(job_ids, check_interval=60):\n",
    "    \"\"\"Wait for all jobs to complete (either SUCCEEDED or FAILED)\"\"\"\n",
    "    \n",
    "    print(f\"Waiting for {len(job_ids)} job(s) to complete...\")\n",
    "    print(\"This may take several minutes depending on the workload.\\n\")\n",
    "    \n",
    "    completed_jobs = set()\n",
    "    \n",
    "    while len(completed_jobs) < len(job_ids):\n",
    "        for job_id in job_ids:\n",
    "            if job_id in completed_jobs:\n",
    "                continue\n",
    "                \n",
    "            response = batch_client.describe_jobs(jobs=[job_id])\n",
    "            job = response['jobs'][0]\n",
    "            \n",
    "            job_name = job['jobName']\n",
    "            status = job['status']\n",
    "            \n",
    "            if status in ['SUCCEEDED', 'FAILED']:\n",
    "                if job_id not in completed_jobs:\n",
    "                    print(f\"Job {job_name} completed with status: {status}\")\n",
    "                    completed_jobs.add(job_id)\n",
    "                    \n",
    "                    if status == 'FAILED' and 'statusReason' in job:\n",
    "                        print(f\"  Failure reason: {job['statusReason']}\")\n",
    "            else:\n",
    "                print(f\"Job {job_name} is {status}\")\n",
    "        \n",
    "        if len(completed_jobs) < len(job_ids):\n",
    "            print(f\"[waiting {check_interval} seconds before next check...]\")\n",
    "            time.sleep(check_interval)\n",
    "            print()\n",
    "    \n",
    "    print(f\"\\n All {len(job_ids)} job(s) have completed!\")\n",
    "    \n",
    "    # Final status summary\n",
    "    print(\"\\nFinal Status Summary:\")\n",
    "    for job_id in job_ids:\n",
    "        response = batch_client.describe_jobs(jobs=[job_id])\n",
    "        job = response['jobs'][0]\n",
    "        print(f\"  {job['jobName']}: {job['status']}\")\n",
    "\n",
    "# Extract job IDs from submitted jobs\n",
    "job_ids = [job['jobId'] for job in jobs]\n",
    "\n",
    "# Wait for all jobs to complete\n",
    "wait_for_jobs_completion(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: View generated sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Download FASTA files with generated sequences from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3_output_path ./data/$LAB1_FOLDER/ --recursive --exclude \"*\" --include \"*.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Read FASTA file(s) and print generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "path = f\"data/{LAB1_FOLDER}\"\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".fasta\"):\n",
    "\n",
    "        file_path = os.path.join(path, file)    \n",
    "        for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "            print(f\"ID: {record.id}\")\n",
    "            print(f\"Description: {record.description}\")\n",
    "            print(f\"Sequence: {record.seq}\")\n",
    "            print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
