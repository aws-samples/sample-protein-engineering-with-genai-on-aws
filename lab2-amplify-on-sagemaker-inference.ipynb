{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 (inference): Generate protein embeddings with AMPLIFY on Amazon SageMaker and filter by quality\n",
    "\n",
    "This notebook will guide you through running inference against the AMPLIFY model on Amazon SageMaker to generate protein embeddings, and using these embeddings to filter low-quality sequences generated by the Progen2 model in Lab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "First, let's get our AWS account information and set up variables we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from sagemaker.pytorch import PyTorchPredictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Get AWS account information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Define S3 bucket and folder names\n",
    "S3_BUCKET = f'workshop-data-{account_id}'\n",
    "LAB1_FOLDER = 'lab1-progen'\n",
    "LAB2_FOLDER = 'lab2-amplify'\n",
    "LAB3_FOLDER = 'lab3-esmfold'\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")\n",
    "\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate protein embeddings using the Amplify model\n",
    "\n",
    "We will generate protein embeddings for the reference protein sequence and all protein sequences generated by the Progen2 model in Lab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1:  Load the reference sequence and sequences generated by the Progen2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference protein sequence\n",
    "record = next(SeqIO.parse('./data/reference.fasta', \"fasta\"))\n",
    "ref_sequence = str(record.seq)\n",
    "ref_embeddings = None\n",
    "\n",
    "\n",
    "# Load protein sequences generated by Progen2 model\n",
    "gen_sequences = []\n",
    "\n",
    "for file in os.listdir(f'./data/{LAB1_FOLDER}'):\n",
    "    if file.endswith(\".fasta\"):\n",
    "        file_path = os.path.join(f'./data/{LAB1_FOLDER}', file)    \n",
    "        for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "            gen_sequences.append({\n",
    "                'prompt_id': record.id,\n",
    "                'sequence': str(record.seq),\n",
    "                'description': record.description,\n",
    "                'embeddings': None,\n",
    "                'distance': None,\n",
    "                'distance_type': None\n",
    "            })\n",
    "\n",
    "\n",
    "print(f'Reference sequence: {ref_sequence}')\n",
    "print()\n",
    "print('Generated sequences:')\n",
    "gen_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Initialize Amazon SageMaker predictor\n",
    "\n",
    "The predictor is a client interface that connects to the deployed AMPLIFY model endpoint, enabling real-time inference calls with JSON serialization for input data and automatic deserialization of model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SageMaker endpoint name for the Amplify model with embeddings\n",
    "endpoint_name_embeddings = 'amplify-120m-endpoint-embeddings'\n",
    "\n",
    "# Initialize predictor using the endpoint name\n",
    "predictor = PyTorchPredictor(\n",
    "    endpoint_name=endpoint_name_embeddings,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Generate embeddings using predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the reference sequence\n",
    "output = predictor.predict({\n",
    "        \"sequence\": ref_sequence, \n",
    "        'mode': 'embeddings'\n",
    "    })\n",
    "ref_embeddings = np.array(output['embeddings'])\n",
    "\n",
    "\n",
    "# Generate embeddings for novel sequences\n",
    "for gen_seq in gen_sequences:\n",
    "    output = predictor.predict({\n",
    "        \"sequence\": gen_seq['sequence'], \n",
    "        'mode': 'embeddings'\n",
    "    })\n",
    "    gen_seq['embeddings'] = np.array(output['embeddings'])\n",
    "    \n",
    "    print(f'Generated sequence : embeddings shape = {gen_seq[\"embeddings\"].shape} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each generated protein sequence is now associated with its corresponding embeddings\n",
    "gen_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter out protein sequences using generated embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Calculate cosine distance \n",
    "Cosine distance is calculated between embeddings of generated and reference protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def cosine_distance(embeddings1, embeddings2, mean_pooling=False):\n",
    "    if mean_pooling:\n",
    "        embeddings1 = embeddings1[:, 1:-1, :].mean(axis=1)\n",
    "        embeddings2 = embeddings2[:, 1:-1, :].mean(axis=1)\n",
    "\n",
    "    return distance.cosine(embeddings1.ravel(), embeddings2.ravel())\n",
    "\n",
    "# Calculate cosine distances for each generated sequences\n",
    "for gen_seq in gen_sequences:\n",
    "    gen_seq['distance'] = cosine_distance(gen_seq['embeddings'], ref_embeddings, mean_pooling=True)\n",
    "    gen_seq['distance_type'] = 'cosine' \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Sort the generated protein sequences by cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gen_sequences)\n",
    "df.set_index('prompt_id', inplace=True)\n",
    "df = df.sort_values(by='distance',ascending=True)\n",
    "df[['distance', 'sequence', 'description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Select five top sequences for the downstream analysis and save them in a FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of sequence records\n",
    "records = []\n",
    "for prompt_id, row in df.head(5).iterrows():\n",
    "    record = SeqRecord(\n",
    "        Seq(row.sequence),\n",
    "        id=prompt_id,\n",
    "        description=f'{row.description},distance={row.distance}'\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "# Save the sequences in a  FASTA file\n",
    "with open(f'./data/{LAB2_FOLDER}/top_sequence_candidates.fasta', 'w') as f:\n",
    "    SeqIO.write(records, f, \"fasta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: [Optional] Delete the unused endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove endpoint\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "try:\n",
    "    sagemaker.delete_endpoint(EndpointName=endpoint_name_embeddings)\n",
    "    print(f\"Successfully deleted endpoint: {endpoint_name_embeddings}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
