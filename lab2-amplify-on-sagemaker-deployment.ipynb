{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 (deployment): Provision resources for running the AMPLIFY model on Amazon SageMaker\n",
    "\n",
    "This notebook will guide you through setting up Amazon SageMaker infrastructure to generate protein embeddings with the AMPLIFY model. We'll create all necessary resources step by step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "First, let's get our AWS account information and set up variables we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Get AWS account information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Define S3 bucket and folder names\n",
    "S3_BUCKET = f'workshop-data-{account_id}'\n",
    "LAB1_FOLDER = 'lab1-progen'\n",
    "LAB2_FOLDER = 'lab2-amplify'\n",
    "LAB3_FOLDER = 'lab3-esmfold'\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")\n",
    "\n",
    "##########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the variables and resources required to deploy the Amplify model on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Define the version of the Amplify model, instance type, and execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model_version = 'AMPLIFY_120M'\n",
    "model_id = f'chandar-lab/{model_version}'\n",
    "\n",
    "# Define instance type\n",
    "instance_type = 'ml.c6i.xlarge'\n",
    "\n",
    "# Retrieve execution role\n",
    "execution_role = get_execution_role()\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"EC2 Instance Type: {instance_type}\") \n",
    "print(f\"Role: {execution_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Retrieve the URI of a base container image\n",
    "This base image will be used by Amazon SageMaker for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_uris.retrieve(\n",
    "    region=region, \n",
    "    framework='pytorch', \n",
    "    image_scope='inference', \n",
    "    version='2.3', \n",
    "    base_framework_version='pytorch2.0.0', \n",
    "    instance_type=instance_type)\n",
    "\n",
    "print(f'Image URI: {image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Create and upload model artifact\n",
    "Package model files into a .tar.gz archive and upload to S3 for SageMaker endpoint deployment. In our case, the model artifact is \n",
    "empty since the model will be downloaded from Hugging Face during container startup. However, if you were training or fine-tuning the\n",
    "model, this artifact would contain your custom model weights and configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_location = f\"s3://{S3_BUCKET}/{LAB2_FOLDER}/model_data.tar.gz\"\n",
    "\n",
    "!mkdir data/$LAB2_FOLDER/model_data\n",
    "!touch data/$LAB2_FOLDER/model_data/data.txt  \n",
    "!tar -czvf data/$LAB2_FOLDER/model_data.tar.gz data/$LAB2_FOLDER/model_data/  \n",
    "!aws s3 cp data/$LAB2_FOLDER/model_data.tar.gz $model_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Create Custom Inference Script and requirements\n",
    "\n",
    "The custom inference script has 4 methods defining how to:\n",
    "* ```model_fn``` - load the model and tokenizer\n",
    "* ```input_fn``` - pre-processes the input data\n",
    "* ```predict_fn``` - tokenize the input protein sequence and run inference on the model\n",
    "* ```output_fn``` - post-processes the output, returning the model's predictions\n",
    "\n",
    "The custom script file and requirements should be stored locally and will be used in the deployment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/$LAB2_FOLDER/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/$LAB2_FOLDER/code/inference-embeddings.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json  \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    logging.info(\"[custom] model_fn: Starting the model loading process...\")\n",
    "\n",
    "    try:\n",
    "        model_id = os.getenv('AMPLIFY_MODEL_ID', 'chandar-lab/AMPLIFY_120M')\n",
    "        logging.info(f\"[custom] model_fn: Model id is {model_id}\")\n",
    "\n",
    "        model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "        logging.info(f\"[custom] model_fn: Successfully loaded the model: {model}\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        logging.info(f\"[custom] model_fn: Successfully loaded the tokenizer: {tokenizer}\")\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        logging.info(f\"[custom] model_fn: Moved model to {device} device\")\n",
    "\n",
    "        return model, tokenizer, device\n",
    "\n",
    "    except Exception as e:        \n",
    "        logging.error(f\"[custom] model_fn: Error occurred while loading the model and tokenizer: {str(e)}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "def input_fn(request_body, content_type='application/json'):\n",
    "    logging.info(\"input_fn: Received input\")\n",
    "    if content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)  \n",
    "        sequence = input_data['sequence']\n",
    "        mode = input_data.get('mode','logits')\n",
    "        return sequence, mode\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    logging.info(\"predict_fn: Running inference\")\n",
    "    sequence, mode = input_data\n",
    "    model, tokenizer, device = model_artifacts\n",
    "    \n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if mode == 'logits':\n",
    "            output = model(inputs)\n",
    "        elif mode == 'embeddings':\n",
    "            output = model(inputs, output_hidden_states=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")            \n",
    "\n",
    "    return output, mode\n",
    "\n",
    "def output_fn(prediction, accept='application/json'):\n",
    "    logging.info(\"output_fn: Formatting output\")\n",
    "    output, mode = prediction\n",
    "    \n",
    "    if accept == 'application/json':\n",
    "        if mode == 'logits':\n",
    "            if hasattr(output, 'logits'):\n",
    "                result = output.logits\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction format: {type(output)}\")\n",
    "        elif mode == 'embeddings':\n",
    "            if hasattr(output, 'hidden_states'):\n",
    "                result = output.hidden_states[-1]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction format: {type(output)}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "        return json.dumps({mode: result.tolist()}), accept\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported accept type: {accept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/$LAB2_FOLDER/code/requirements.txt\n",
    "transformers==4.37.0\n",
    "xformers==0.0.28.post1\n",
    "sentencepiece==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Deploy and test the Amplify model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Deploy the Amplify model with PyTorchModel and Custom Inference Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SageMaker endpoint name\n",
    "base_name = model_id.split(\"/\")[-1].lower().replace(\"_\", \"-\").replace(\".\", \"-\")\n",
    "endpoint_name = f\"{base_name}-endpoint\"\n",
    "\n",
    "print(f'Endpoint name: {endpoint_name}')\n",
    "\n",
    "# Create endpoint definition\n",
    "sm_model = PyTorchModel(\n",
    "    model_data=model_data_location,\n",
    "    role=execution_role,\n",
    "    image_uri=image,\n",
    "    source_dir=f'data/{LAB2_FOLDER}/code/',\n",
    "    entry_point=\"inference.py\",\n",
    "    env={\n",
    "        'AMPLIFY_MODEL_ID': model_id\n",
    "    }\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Test the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()  \n",
    "predictor.deserializer = JSONDeserializer()  \n",
    "\n",
    "sequence = \"MSVVGIDLGFQSCYVAVARAGGIETIANEYSDRCTPACISFGPKNR\"\n",
    "\n",
    "# generate logits\n",
    "result = predictor.predict({\"sequence\": sequence, \"mode\": \"logits\"})\n",
    "logits = np.array(result['logits'])\n",
    "\n",
    "print(result)\n",
    "print(f'Logits shape: {logits.shape}')\n",
    "print()\n",
    "\n",
    "# generate embeddings\n",
    "result = predictor.predict({\"sequence\": sequence, \"mode\": \"embeddings\"})\n",
    "embeddings = np.array(result['embeddings'])\n",
    "print(result)\n",
    "print(f'Embeddings shape: {embeddings.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
